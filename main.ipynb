{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Musik Projekt",
   "id": "59f3ce0f425630fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data import",
   "id": "a2647efe72170a8d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T19:14:44.739997Z",
     "start_time": "2025-04-22T19:14:44.733851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder"
   ],
   "id": "c2b28a54efd88b99",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set random seed",
   "id": "a3029c8f80e401dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T19:14:44.785878Z",
     "start_time": "2025-04-22T19:14:44.765163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Set seed for PyTorch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set seed for NumPy\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set seed for Python's random module\n",
    "random.seed(42)\n",
    "\n",
    "# If you are using CUDA\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)  # if you are using multi-GPU\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ],
   "id": "f1e987492c829937",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T19:14:44.802134Z",
     "start_time": "2025-04-22T19:14:44.795879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# import the modules\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "# get the path/directory\n",
    "folder_dir = \"C:/Users/thimo/Documents/FS25_ZHAW/Neural_Networks_and_Deep_Learning/pyscripts/pythonProject/project/archive/Data/images_original/blues\"\n",
    "for images in os.listdir(folder_dir):\n",
    "    # check if the image ends with png\n",
    "    if (images.endswith(\".png\")):\n",
    "        print(images)"
   ],
   "id": "1bd172acdac9694",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blues00000.png\n",
      "blues00001.png\n",
      "blues00002.png\n",
      "blues00003.png\n",
      "blues00004.png\n",
      "blues00005.png\n",
      "blues00006.png\n",
      "blues00007.png\n",
      "blues00008.png\n",
      "blues00009.png\n",
      "blues00010.png\n",
      "blues00011.png\n",
      "blues00012.png\n",
      "blues00013.png\n",
      "blues00014.png\n",
      "blues00015.png\n",
      "blues00016.png\n",
      "blues00017.png\n",
      "blues00018.png\n",
      "blues00019.png\n",
      "blues00020.png\n",
      "blues00021.png\n",
      "blues00022.png\n",
      "blues00023.png\n",
      "blues00024.png\n",
      "blues00025.png\n",
      "blues00026.png\n",
      "blues00027.png\n",
      "blues00028.png\n",
      "blues00029.png\n",
      "blues00030.png\n",
      "blues00031.png\n",
      "blues00032.png\n",
      "blues00033.png\n",
      "blues00034.png\n",
      "blues00035.png\n",
      "blues00036.png\n",
      "blues00037.png\n",
      "blues00038.png\n",
      "blues00039.png\n",
      "blues00040.png\n",
      "blues00041.png\n",
      "blues00042.png\n",
      "blues00043.png\n",
      "blues00044.png\n",
      "blues00045.png\n",
      "blues00046.png\n",
      "blues00047.png\n",
      "blues00048.png\n",
      "blues00049.png\n",
      "blues00050.png\n",
      "blues00051.png\n",
      "blues00052.png\n",
      "blues00053.png\n",
      "blues00054.png\n",
      "blues00055.png\n",
      "blues00056.png\n",
      "blues00057.png\n",
      "blues00058.png\n",
      "blues00059.png\n",
      "blues00060.png\n",
      "blues00061.png\n",
      "blues00062.png\n",
      "blues00063.png\n",
      "blues00064.png\n",
      "blues00065.png\n",
      "blues00066.png\n",
      "blues00067.png\n",
      "blues00068.png\n",
      "blues00069.png\n",
      "blues00070.png\n",
      "blues00071.png\n",
      "blues00072.png\n",
      "blues00073.png\n",
      "blues00074.png\n",
      "blues00075.png\n",
      "blues00076.png\n",
      "blues00077.png\n",
      "blues00078.png\n",
      "blues00079.png\n",
      "blues00080.png\n",
      "blues00081.png\n",
      "blues00082.png\n",
      "blues00083.png\n",
      "blues00084.png\n",
      "blues00085.png\n",
      "blues00086.png\n",
      "blues00087.png\n",
      "blues00088.png\n",
      "blues00089.png\n",
      "blues00090.png\n",
      "blues00091.png\n",
      "blues00092.png\n",
      "blues00093.png\n",
      "blues00094.png\n",
      "blues00095.png\n",
      "blues00096.png\n",
      "blues00097.png\n",
      "blues00098.png\n",
      "blues00099.png\n"
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T19:14:44.923523Z",
     "start_time": "2025-04-22T19:14:44.817237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "input_dir = \"C:/Users/thimo/Documents/FS25_ZHAW/Neural_Networks_and_Deep_Learning/pyscripts/pythonProject/project/archive/Data/images_original/blues\"\n",
    "\n",
    "# load images one after another - useful for images one by one\n",
    "for image in os.listdir(input_dir):\n",
    "    img = Image.open(os.path.join(input_dir, image))\n",
    "    # do whatever you like to do with the img\n",
    "\n",
    "# load all images data at in to list\n",
    "image_list = [Image.open(os.path.join(input_dir, image)) for image in os.listdir(input_dir)]\n"
   ],
   "id": "8df3723e5179d135",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_dir = \"C:/Users/thimo/Documents/FS25_ZHAW/Neural_Networks_and_Deep_Learning/pyscripts/pythonProject/project/archive/Data/images_original/blues\"\n",
    "\n",
    "# Load images one after another - useful for images one by one\n",
    "for image in os.listdir(input_dir):\n",
    "    img = Image.open(os.path.join(input_dir, image))\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')  # Hide axes\n",
    "    plt.show()\n",
    "\n",
    "# Load all images data into a list\n",
    "image_list = [Image.open(os.path.join(input_dir, image)) for image in os.listdir(input_dir)]\n",
    "\n",
    "# Print all images in the list\n",
    "for img in image_list:\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')  # Hide axes\n",
    "    plt.show()"
   ],
   "id": "cc14ace1f0de9f62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T19:15:05.806862Z",
     "start_time": "2025-04-22T19:15:05.758279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"andradaolteanu/gtzan-dataset-music-genre-classification\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ],
   "id": "204ad524e4f21632",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kagglehub'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[67]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mkagglehub\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;66;03m# Download latest version\u001B[39;00m\n\u001B[32m      4\u001B[39m path = kagglehub.dataset_download(\u001B[33m\"\u001B[39m\u001B[33mandradaolteanu/gtzan-dataset-music-genre-classification\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'kagglehub'"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Split the datafolder into test and training data (not equal across genres yet!) -> unbalanced",
   "id": "782ca1dfdeaeb670"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T19:15:05.842870300Z",
     "start_time": "2025-04-22T18:26:42.102216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# Data directory\n",
    "data_dir = \"C:/Users/thimo/Documents/FS25_ZHAW/Neural_Networks_and_Deep_Learning/pyscripts/pythonProject/project/archive/Data/images_original\"\n",
    "\n",
    "\n",
    "# Load the dataset with original image size\n",
    "dataset = ImageFolder(data_dir, transform=transforms.Compose([transforms.Resize((288, 432)), # Adjust to original image size\n",
    " transforms.ToTensor()]))\n",
    "\n",
    "# Load the dataset\n",
    "# dataset = ImageFolder(data_dir, transform=transforms.Compose([\n",
    "#     transforms.Resize((150, 150)),\n",
    "#     transforms.ToTensor()]))\n",
    "\n",
    "# Define the split ratio\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Calculate the number of samples for training and testing\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# Randomly split the dataset\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders (load the train and validation into batches)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Print the number of samples in each set\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of testing samples: {len(test_dataset)}\")\n",
    "print(train_dataset)"
   ],
   "id": "3d0de942c500c64a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 799\n",
      "Number of testing samples: 200\n",
      "<torch.utils.data.dataset.Subset object at 0x000001B2E4F20850>\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Look at first 5 images of training dataset (function proposed by microsoft copilot)",
   "id": "397885b6bea6e4ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to show an image\n",
    "def show_image(img, title=None):\n",
    "    img = img.permute(1, 2, 0)  # Change the order of dimensions\n",
    "    plt.imshow(img)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display a few images from the training dataset\n",
    "for i in range(5):  # Change the range to display more or fewer images\n",
    "    img, label = train_dataset[i]\n",
    "    show_image(img, title=f\"Label: {train_dataset.dataset.classes[label]}\")\n"
   ],
   "id": "774f496fdc0d823a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Recall tensor shape",
   "id": "bcb9199311fd0f7e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "img, label = dataset[0]\n",
    "print(img.shape, label)\n",
    "img, label = train_dataset[0]\n",
    "print(img.shape, label) # prints size of first image: 3 channels (RGB), height, width\n",
    "show_image(img, title=f\"Label: {train_dataset.dataset.classes[label]}\")\n",
    "\n",
    "#output :\n",
    "#torch.Size([3, 288, 432]) 0 # has label 0 which represents genre blues,\n",
    "#torch.Size([3, 288, 432]) 8 # first image of train ddset (8 = raggae)"
   ],
   "id": "5ca635f8e8ccc13f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "See all 10 music genres (classes) of dataset",
   "id": "132cf098470029fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T19:15:05.864406200Z",
     "start_time": "2025-04-22T18:30:21.884683Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"Follwing classes are there : \\n\",dataset.classes)",
   "id": "97f857413b43250",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Follwing classes are there : \n",
      " ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Different printing function based on tutorial (Medium article)",
   "id": "3a1f6b3f545eeb59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def display_img(img,label):\n",
    "    print(f\"Label : {dataset.classes[label]}\")\n",
    "    plt.imshow(img.permute(1,2,0))\n",
    "\n",
    "#display the first image in the dataset\n",
    "display_img(*dataset[0])"
   ],
   "id": "dd5bf3cc23bfc7e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualize training batch (batch size 32 see above)",
   "id": "d3f815292a228f76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_batch(dl):\n",
    "    \"\"\"Plot images grid of single batch\"\"\"\n",
    "    for images, labels in dl:\n",
    "        fig,ax = plt.subplots(figsize = (16,12))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images,nrow=8).permute(1,2,0))\n",
    "        break\n",
    "\n",
    "show_batch(train_loader)"
   ],
   "id": "ed661a421a1935e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Base Model for Image Classification",
   "id": "6c63aa65c5616f3c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This base class is used to develop all neural networks. We add functionalities to the base to train the model, validate the model and get the result for each epoch. It's reusable and can be used for any image classification model, no need to rewrite this every time.",
   "id": "e929e53a6cf5a7c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T19:15:56.548270Z",
     "start_time": "2025-04-22T19:15:56.541054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        acc = accuracy(out, labels)         # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()  # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()     # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['train_loss'],result['val_loss'], result['val_acc']))"
   ],
   "id": "1bac8c7ccf393b14",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Concepts to refine the model: Convolution, Padding, Stride, Maxpooling",
   "id": "405a563519867d0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### CNN Model for Classification",
   "id": "fc419e17e5cb43bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Final model with 3 CNN blocks, each consisting of 2 convolutional layers & 1 max-pooling layer. Relu activation function is used to remove negative values from the feature map. Stride(1, 1), padding = 1.After applying convolution and feature extracting a flatten layer is used to flatten the tensor from 3 to 1 dimension. Then 3 linear layers are added to reduce the size of the tensor and learn the features.",
   "id": "77ab9a7347adca4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T19:57:14.318275Z",
     "start_time": "2025-04-22T19:57:14.307442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class MusicGenreClassification(ImageClassificationBase):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(3, 32, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32,64, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128 ,128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256,256, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(497664,1024), # because our images are not 150x150 first number isnot 82944\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10) # we have 10 classes and not 6\n",
    "        )\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ],
   "id": "1db1c57635b98d01",
   "outputs": [],
   "execution_count": 87
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Hyperparameters, Model Training, And Evaluation:\n",
    "Now we have to train the music genre classification model on the training dataset. That defines the fit, evaluation and accuracy methods."
   ],
   "id": "93e54a799869c2fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T19:57:18.196707Z",
     "start_time": "2025-04-22T19:57:18.186530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(),lr)\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "\n",
    "    return history"
   ],
   "id": "25b057731aa10561",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-22T19:57:20.445110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Instantiate the model\n",
    "model = MusicGenreClassification()\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 30\n",
    "\n",
    "# Optimizer\n",
    "opt_func = torch.optim.Adam\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# fitting the model on training data and record the result after each epoch\n",
    "history = fit(num_epochs, lr, model, train_loader, test_loader, opt_func) # in online article they used val_loader (validation) instead of test_loader"
   ],
   "id": "bab06fbdd4f60346",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T19:57:02.236860Z",
     "start_time": "2025-04-22T19:57:02.229831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check the number of classes in the dataset\n",
    "num_classes = len(dataset.classes)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ],
   "id": "fb5a95fa79d9632b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 10\n"
     ]
    }
   ],
   "execution_count": 86
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
